import os

import pandas as pd
data = pd.read_csv('clean r.csv')
data.head()

data.shape 

print("Number off null in text:{}".format(data['text'].isnull().sum()))

data = data.dropna()

data['text'] = [entry.lower() for entry in data['text']]
data['text'].head()

import string
string.punctuation

#Remove Punctuation
def remove_punct(text):
    text_nopunct = "".join([char for char in text if char not in string.punctuation])# It will discard all punctuations
    return text_nopunct

data['text'] = data['text'].apply(lambda x: remove_punct(x))
data['text'].head()   

#remove angka
data['text']=data['text'].str.replace(r'[\d+]', '')
data['text']

#link https://github.com/har07/PySastrawi
!pip install Sastrawi

#stopword sastrawi di link  'https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/'
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
 
factory = StopWordRemoverFactory()
stopwords = factory.get_stop_words()
print(stopwords)

# import StopWordRemoverFactory class
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
 
factory = StopWordRemoverFactory()
stopword =  factory.get_stop_words()
 
# Kalimat
data['text'] = data['text'].apply(lambda x: " ".join(x for x in x.split() if x not in stopword))
data['text'].head()

import requests
def stopwords():
    r = requests.get("https://raw.githubusercontent.com/masdevid/ID-Stopwords/master/id.stopwords.02.01.2016.txt").text
    data = []
    for x in r.split("\n"):
        data.append(x)
    return data

stopwords()

# Import Stopword Factory class
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

#Create factory
factory = StopWordRemoverFactory()
more_stopword = ['pls', 'bgt', 'nahh', 'tuh','iya','erti','york','yasudah','yefedu','yoi','yongcry','yorr','yu','ace','adhityasandi','adro','aali','aan','aces','acu','acung','adhi','adhima','ssma ',' breadtalkindo','yudha','yudhakhel','yudhamology','yuwono',
                'zinc','zipmexarticle','zonk','ora','or','aamiin','nya','kali','sih','yg','tidaknya','mah','lah','kok','yah','bah','bro','sis','sist','dong','tuh','oh','begitu'
                ,'pas','gak','dongs','hehe','haha','hihi','dih','duh','wah','ya','yah','eh','deh','dah','wkwk','wow','si','guys','gaes','mba','mbak','mas','afl','xi','wowjepang'
                ,'bang','kak','mah','yuk','bet','loh','woy','doang','tuh','euy','doi','yuhu','ada','aku','gw','gue','gua','elu','lu','kamu','yang','kalau','agam','xl'
                ,'jos','abai','abdul','jokowi','abdullah','ac','kita','gitu','aja','saja','abang','ah','ih','hey','hai','pubg','jt','om','nana','najwa','keke','yak','yan'
                ,'ih','woi','ew','zoe','lur','da','year','aa','yuk','achmad' ,'pakde','uwu','tos','po','jokowi','raffi', 'ahmad','yoga','yangs','yaa','yaelah','yam', 'kayak','crying','face']
stopwordplus = factory.get_stop_words()+stopwords()+more_stopword

data['text'] = data['text'].apply(lambda x: " ".join(x for x in x.split() if x not in stopwordplus))
data['text']

stopwordplus

import re
# Function to Tokenize words
def tokenize(text):
    tokens = re.split('\W+', text) #W+ means that either a word character (A-Za-z0-9_) or a dash (-) can go there.
    return tokens
def convertToString(term):
  if type(term) is str:
    return term
  else:
    return str(term)
data['text'] = data['text'].apply(lambda x: tokenize(x.lower())) 
#We convert to lower as Python is case-sensitive. 
data.head()

# import StemmerFactory class
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
# create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()
# stemming process
def kata_stem(teks):
    stem_teks = " ".join([stemmer.stem(i) for i in teks])
    return stem_teks
data['text'] = data['text'].apply(lambda x: kata_stem(x))
data['text'].head()

import re
# Function to Tokenize words
def tokenize(text):
    tokens = re.split('\W+', text) #W+ means that either a word character (A-Za-z0-9_) or a dash (-) can go there.
    return tokens

data['text'] = data['text'].apply(lambda x: tokenize(x.lower())) 
#We convert to lower as Python is case-sensitive. 
data.head()

#simpan dalam bentuk csv
#data.to_csv("PPKM fixxx.csv")

import pandas as pd
data = pd.read_csv("PPKM fixxx.csv", sep=';')
data.head()

from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1, 1), tokenizer=None, analyzer = 'word', stop_words=stopwordplus)
countvec = count_vectorizer.fit_transform(data.text).toarray()
countvec

countvec2 = pd.DataFrame(countvec)
countvec2

kata_kata = count_vectorizer.get_feature_names()
countvec3 = pd.DataFrame(countvec, columns=kata_kata)
countvec3

# Menghitung TF-IDF 

from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer(norm=None, use_idf=True, smooth_idf=False, sublinear_tf=False)
tfidf = transformer.fit_transform(countvec)
tfidf

# Mengubah menjadi array
tfidf1 = tfidf.toarray()
tfidf1

tfidf2 = pd.DataFrame(tfidf1)
tfidf2

kata_kata2 = count_vectorizer.get_feature_names()
df1 = pd.DataFrame(tfidf1, columns=kata_kata2)
df1
#kalau mau disave
#df1.to_csv("df1.csv")

#3.1 Read file as panda dataframe
df = pd.read_csv("PPKM fixxx.csv", sep=';') #create data frame

text = df['text']
text_list = []
for i in range(len(text)) :
    bbb = text[i].replace('[', '')
    bbb = bbb.replace(']', '')
    bbb = bbb.replace("'", "")
    bbb = bbb.replace(",", "")
    temp = []
    for j in bbb.split() :
        temp.append(j)
    text_list.append(temp)

print(len(text_list))

df.head()

print(text_list)

pip install -U gensim 

#Create Bigram & Trigram Models 
from gensim.models import Phrases
# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.
bigram = Phrases(text_list, min_count=10)
trigram = Phrases(bigram[text_list])

for idx in range(len(text_list)):
    for token in bigram[text_list[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            text_list[idx].append(token)
    for token in trigram[text_list[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            text_list[idx].append(token)

from gensim import corpora, models
# Create a dictionary representation of the documents.
dictionary = corpora.Dictionary(text_list)

dictionary.filter_extremes(no_below=5, no_above=0.2) 
#no_below (int, optional) – Keep tokens which are contained in at least no_below documents.
#no_above (float, optional) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number).

print(dictionary)

#https://radimrehurek.com/gensim/tut1.html 
#build corpus

doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]
#The function doc2bow converts document (a list of words) into the bag-of-words format
print(len(doc_term_matrix))
print(doc_term_matrix[100])

tfidf = models.TfidfModel(doc_term_matrix) #build TF-IDF model
corpus_tfidf = tfidf[doc_term_matrix]

from gensim.models.coherencemodel import CoherenceModel
from gensim.models.ldamodel import LdaModel
from gensim.corpora.dictionary import Dictionary
from numpy import array
#function to compute coherence values
def compute_coherence_values(dictionary, corpus, texts, limit, start, step):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())
        
    return model_list, coherence_values

start=1
limit=11
step=1
model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf, 
                                                        texts=text_list, start=start, limit=limit, step=step)
#show graphs
import matplotlib.pyplot as plt
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()

# Print the coherence scores
for m, cv in zip(x, coherence_values):
    print("Num Topics =", m, " has Coherence Value of", round(cv, 6))

from pprint import pprint

model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=5)
pprint(model.print_topics())

model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=5)

for idx, topic in model.print_topics(-1):
    print('Topic: {} Word: {}'.format(idx, topic))

import pandas as pd
top_words_per_topic = []
for t in range(model.num_topics):
    top_words_per_topic.extend([(t, ) + x for x in model.show_topic(t, topn = 5)])

#pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv("top_words.csv")
df = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word','P']).to_csv("top_words2.csv")
print(df)

!pip install -q pyLDAvis

!pip install pyLDAvis==2.1.2

import gensim
import pyLDAvis.gensim;pyLDAvis.enable_notebook()

data = pyLDAvis.gensim.prepare(model, corpus_tfidf, dictionary)
print(data)
pyLDAvis.save_html(data, 'lda-gensim.html')

import matplotlib.pyplot as plt
from wordcloud import WordCloud as wd

for t in range(model.num_topics):
    plt.figure(figsize=(7,6))
    plt.imshow(wd(max_font_size=50, min_font_size=6).fit_words(dict(model.show_topic(t, 200))))
    plt.axis("off")
    plt.title("Topic #" + str(t))
    plt.savefig("wcld-topic-#"+str(t)+".png", facecolor='k', bbox_inches='tight')

plt.show()

pip install pyldavis

import pyLDAvis
import pyLDAvis.gensim
vis = pyLDAvis.gensim.prepare(model, corpus_tfidf, dictionary)
pyLDAvis.enable_notebook()
pyLDAvis.display(vis)